{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing liraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import math as m\n",
    "#import scipy as sc\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "# NLTK stemmers\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "#import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create an object of class TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to lemmatize each word with its POS tag\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "# # Lemmatize\n",
    "# sentence = \"The striped bats are hanging on their feet for best\"\n",
    "# lemmatize_with_postag(sentence)\n",
    "# #> 'The striped bat be hang on their foot for best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of acronyms will be presumably captured by n=3\n",
    "# longer ngrams, such as n=6, retain more of the word\n",
    "def ngramsX(string, n):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# #create an object of class LancasterStemmer\n",
    "# lancaster=LancasterStemmer()\n",
    "\n",
    "#create an object of class SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_list(mylist):\n",
    "    \n",
    "    sentences = mylist.copy()\n",
    "    sentences_index = 0  \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        #print('') # Diagnostic\n",
    "        #print(sentence)\n",
    "        \n",
    "        # -------------------------------------\n",
    "        # A. Basic pre-processing, regex-style\n",
    "        # -------------------------------------\n",
    "        \n",
    "        # create word tokens\n",
    "        xd = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # lower case. NLTK lemmatizer is case-sensitive\n",
    "        xd = [str.lower(word) for word in xd]\n",
    "        \n",
    "        # Do NOT split terms like 'r/t', 'f/u' ...\n",
    "        # ... but DO split compound words such as 'cancer/colostomy'\n",
    "        xd = [word.split(\"/\") for word in xd if (len(word) >= 3) ]\n",
    "        \n",
    "        # The above turns xd into a list of lists, so flatten it\n",
    "        xd = [item for sublist in xd for item in sublist]\n",
    "        \n",
    "        # Exclude single-characters and stop-words from word tokens\n",
    "        xd = [word for word in xd if (len(word) > 1) & (not word in stop_words) ]\n",
    "        # Still retained - words such as, 'r/t', 'f/u', 'pt', 'dx', 'tx', 'rx', \"'s\", \"mass/\", '2017'\n",
    "\n",
    "        # Remove non-ASCII\n",
    "        xd = [re.sub(r'[^\\x00-\\x7F]+',' ', word) for word in xd]\n",
    "\n",
    "        # Remove words that start with numbers\n",
    "        xd = [word for word in xd if not re.search('^[0-9]', word) ]\n",
    "        #print(xd) # Diagnostic\n",
    "        \n",
    "        # -------------------------------------\n",
    "        # B. NLP lemmatization & stemming\n",
    "        # -------------------------------------\n",
    "        \n",
    "        # Lemmatize a Sentence with the appropriate POS tag\n",
    "        lemmatized_sentence = [lemmatize_with_postag(word)for word in xd]\n",
    "        #print(lemmatized_sentence)  #Diagnostic\n",
    "        \n",
    "        # Porter, Lancaster, and SnowBall stemmers\n",
    "        xd_porter_stemmed = [porter.stem(word) for word in lemmatized_sentence]\n",
    "        #xd_lancaster_stemmed = [lancaster.stem(word) for word in lemmatized_sentence]\n",
    "        xd_snowball_stemmed = [snowball.stem(word) for word in lemmatized_sentence]\n",
    "        #print(xd_porter_stemmed)\n",
    "        \n",
    "        \n",
    "        # Feature space w/ ngrams\n",
    "        ll = [ lemmatized_sentence, \n",
    "              xd_porter_stemmed, #xd_lancaster_stemmed, \n",
    "              xd_snowball_stemmed,\n",
    "              ngramsX(' '.join(xd),3), ngramsX(' '.join(xd),4), ngramsX(' '.join(xd),6)\n",
    "             ]\n",
    "        \n",
    "        # Flatten the list of lists, ll\n",
    "        flat_list = [item for sublist in ll for item in sublist]\n",
    "        \n",
    "        sentence_2 = ' '.join(flat_list)\n",
    "        #print(sentence_2)\n",
    "        \n",
    "        # Update list element\n",
    "        sentences[sentences_index] = sentence_2\n",
    "        \n",
    "        # Increment counter\n",
    "        sentences_index = sentences_index + 1\n",
    "        \n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get matches for problems with threshold 0.4\n",
    "def get_matches_oppo(search,data,action_list):\n",
    "   \n",
    "    # Both sample pre processing\n",
    "    o2 = preprocess_list(search)\n",
    "    p2 = preprocess_list(data)\n",
    "    \n",
    "    docs = [item for sublist in [o2,p2] for item in sublist]\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "    # #cosine similarity\n",
    "    # # The diagonal elements of the cosine similarity matrix equal 1 (cosine similarity of a string to itself)\n",
    "    # # We don't care for those, simply subtract them out using np.eye()  \n",
    "    cs = (cosine_similarity(tfidf_matrix, tfidf_matrix) - np.eye(tfidf_matrix.shape[0])).round(3)\n",
    "    # create an Empty DataFrame object \n",
    "    df2 = pd.DataFrame()\n",
    "    # Create rows\n",
    "    df2['Problems/deficit'] = search\n",
    "    # Fill-in the remaining columns\n",
    "    col_index = 0\n",
    "    for i in data:\n",
    "        df2[i] = cs[0:len(o2) , (len(o2) + col_index):((len(o2) + col_index)+1)]\n",
    "        col_index = col_index + 1\n",
    "\n",
    "\n",
    "    row_list1=action_list\n",
    "    row_list_index=[i for i in range(len(row_list1))]\n",
    "\n",
    "    row_dict_map=dict(zip(row_list_index,row_list1))\n",
    "    \n",
    "    \n",
    "    #threshold\n",
    "    df2=df2[df2.iloc[:,1:len(df2.iloc[0])] > 0.4]\n",
    "\n",
    "    final_data=  pd.DataFrame(df2[df2.iloc[:,1:len(df2.iloc[0])]!=0].stack())\n",
    "\n",
    "    final_data.reset_index(inplace=True)\n",
    "    final_data.columns=['index','Opportunities','Score']\n",
    "#     final_data.set_index('index')\n",
    "    final_data['actions']=final_data['index'].map(row_dict_map)\n",
    "    #return final_data\n",
    "#     final_data.reset_index(inplace=True)\n",
    "#     final_data.columns=['Opportunities','Cross_Opportunities','Score']\n",
    "    return final_data.sort_values(by=['Score'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyconnect_py37]",
   "language": "python",
   "name": "conda-env-pyconnect_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
